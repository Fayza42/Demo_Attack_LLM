# pharma_pentest/run_pentest.py
import os
import time
import requests
import matplotlib.pyplot as plt
import seaborn as sns
from adapter import PharmaToZenguardAdapter


def main():
    # Configuration
    api_url = "http://localhost:8000/api/chat"
    lakera_file_path = "data/lakera_custom_prompts.json"

    # Create output directories if they don't exist
    os.makedirs("results/csv", exist_ok=True)
    os.makedirs("results/visualizations", exist_ok=True)

    # Output files
    output_file_safe = "results/csv/pentest_results_safe.csv"
    output_file_unsafe = "results/csv/pentest_results_unsafe.csv"
    output_file_combined = "results/csv/pentest_results_combined.csv"
    visualization_file = "results/visualizations/pentest_results.png"

    # Create the adapter that links Lakera prompts with zenguard framework
    adapter = PharmaToZenguardAdapter(api_url, lakera_file_path)

    # Load and convert prompts
    print("Loading and converting Lakera prompts...")
    adapter.load_lakera_prompts()
    print(f"Loaded {len(adapter.lakera_prompts)} prompt injection attacks")

    adapter.convert_to_zenguard_format()
    print(f"Converted to {len(adapter.zenguard_prompts)} zenguard-compatible prompts")

    # Gardez la référence à la méthode originale
    original_run_tests = adapter.run_tests

    # Définir la nouvelle fonction avec délai et timeout augmenté
    def run_tests_with_delay(security_mode="SAFE", verbose=False):
        """Version modifiée de run_tests avec délai entre requêtes et timeout augmenté"""
        if not adapter.zenguard_prompts:
            adapter.convert_to_zenguard_format()

        # Make a copy to avoid modifying the original
        prompts_copy = adapter.zenguard_prompts.copy()

        # Counters for debugging
        success_count = 0
        error_count = 0
        timeout_count = 0

        # Run the tests against the API
        total_prompts = len(prompts_copy)
        for i, prompt_data in enumerate(prompts_copy):
            # Get the prompt from the prompt_data
            prompt_str = prompt_data["prompt"]

            print(f"\nTraitement du prompt {i + 1}/{total_prompts}:")
            print(f"Prompt: {prompt_str[:100]}...")

            # Prepare the API request
            api_request = {
                "message": prompt_str,
                "template_type": "sales",
                "user": {
                    "id": "Aimé Millet",
                    "first_name": "Aimé",
                    "last_name": "Millet"
                },
                "security_mode": security_mode
            }

            # Send the request to the API with an increased timeout (10 minutes)
            try:
                print(f"Envoi de la requête... (timeout: 600s)")
                response = requests.post(adapter.api_url, json=api_request, timeout=600)

                # Store the result
                if response.status_code == 200:
                    result = response.json()

                    # Capture the final model response text (this is what we want)
                    final_response = result.get("content", "")

                    # Store both the raw API response and the final text response
                    prompt_data["raw_response"] = result
                    prompt_data["result"] = final_response

                    if final_response:
                        success_count += 1
                        print(f"Succès! Réponse reçue ({len(final_response)} caractères)")
                        if verbose:
                            print(f"Réponse: {final_response[:150]}...")
                    else:
                        error_count += 1
                        print(f"Attention: Réponse vide reçue")
                else:
                    error_msg = f"Erreur API: {response.status_code}"
                    prompt_data["result"] = error_msg
                    error_count += 1
                    print(error_msg)
            except requests.exceptions.Timeout:
                timeout_msg = "Requête expirée après 10 minutes"
                prompt_data["result"] = timeout_msg
                timeout_count += 1
                print(timeout_msg)
            except Exception as e:
                error_msg = f"Exception: {str(e)}"
                prompt_data["result"] = error_msg
                error_count += 1
                print(error_msg)

            # Add delay before next request (except for the last one)
            if i < total_prompts - 1:
                print(f"Attente de 5 secondes avant la prochaine requête...")
                time.sleep(5)  # 5 secondes de délai

        # Print summary
        print(f"\nRésumé du test pour le mode {security_mode}:")
        print(f"  Total des prompts: {total_prompts}")
        print(f"  Réponses réussies: {success_count}")
        print(f"  Timeouts: {timeout_count}")
        print(f"  Autres erreurs: {error_count}")

        # Store the results based on the security mode
        adapter.test_results[security_mode] = prompts_copy

        return prompts_copy

    # Remplacer la méthode run_tests par notre version modifiée
    adapter.run_tests = run_tests_with_delay

    # Run tests in SAFE mode
    print("\nRunning tests in SAFE mode...")
    adapter.run_tests(security_mode="SAFE")

    # Run tests in UNSAFE mode
    print("\nRunning tests in UNSAFE mode...")
    adapter.run_tests(security_mode="UNSAFE")

    # Restore original method
    adapter.run_tests = original_run_tests

    # Score all results
    print("\nScoring test results...")
    adapter.score_results("SAFE")
    adapter.score_results("UNSAFE")

    # Save the results to CSV
    print(f"\nSaving SAFE mode results to {output_file_safe}...")
    adapter.save_results_csv(output_file_safe, security_mode="SAFE")

    print(f"Saving UNSAFE mode results to {output_file_unsafe}...")
    adapter.save_results_csv(output_file_unsafe, security_mode="UNSAFE")

    print(f"Saving combined results to {output_file_combined}...")
    adapter.save_results_csv(output_file_combined)

    # Generate and print a summary
    summary = adapter.generate_summary()

    # Print SAFE mode summary
    safe_summary = summary["SAFE"]
    print("\n=== SAFE MODE TEST SUMMARY ===")
    print(f"Total prompts tested: {safe_summary['total_tests']}")
    print(f"Successful attacks: {safe_summary['successful_attacks']} ({safe_summary['success_rate']:.2f}%)")
    print(f"Average success percentage: {safe_summary['avg_success_percentage']:.2f}%")

    print("\n=== SUCCESS BY ATTACK TYPE (SAFE MODE) ===")
    print(safe_summary["attack_types"])

    print("\n=== SUCCESS BY COMPLEXITY LEVEL (SAFE MODE) ===")
    print(safe_summary["complexity_levels"])

    print("\n=== TOP 5 MOST SUCCESSFUL ATTACKS (SAFE MODE) ===")
    print(safe_summary["top_attacks"].to_string(index=False))

    # Print UNSAFE mode summary
    unsafe_summary = summary["UNSAFE"]
    print("\n=== UNSAFE MODE TEST SUMMARY ===")
    print(f"Total prompts tested: {unsafe_summary['total_tests']}")
    print(f"Successful attacks: {unsafe_summary['successful_attacks']} ({unsafe_summary['success_rate']:.2f}%)")
    print(f"Average success percentage: {unsafe_summary['avg_success_percentage']:.2f}%")

    print("\n=== SUCCESS BY ATTACK TYPE (UNSAFE MODE) ===")
    print(unsafe_summary["attack_types"])

    print("\n=== SUCCESS BY COMPLEXITY LEVEL (UNSAFE MODE) ===")
    print(unsafe_summary["complexity_levels"])

    print("\n=== TOP 5 MOST SUCCESSFUL ATTACKS (UNSAFE MODE) ===")
    print(unsafe_summary["top_attacks"].to_string(index=False))

    # Print comparison summary
    if "comparison" in summary:
        comparison = summary["comparison"]
        print("\n=== SECURITY MODE COMPARISON ===")
        print(f"SAFE mode success rate: {comparison['safe_success_rate']:.2f}%")
        print(f"UNSAFE mode success rate: {comparison['unsafe_success_rate']:.2f}%")
        print(f"Difference: {comparison['difference']:.2f}%")
        print(f"Vulnerability reduction: {comparison['vulnerability_reduction']:.2f}%")

    # Create visualizations
    print(f"\nCreating visualization and saving to {visualization_file}...")
    create_visualization(adapter, visualization_file)

    print("\n=== TESTING COMPLETE ===")


def create_visualization(adapter, output_file):
    """Create visualizations of the test results"""
    # Get the data
    df_safe = adapter.create_results_dataframe("SAFE")
    df_unsafe = adapter.create_results_dataframe("UNSAFE")
    df_combined = adapter.create_results_dataframe()

    # Create a figure with multiple subplots
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # Plot 1: Success Rate by Security Mode
    df_mode = df_combined.groupby("security_mode")["success"].mean().reset_index()
    sns.barplot(x="security_mode", y="success", data=df_mode, ax=axes[0, 0])
    axes[0, 0].set_title("Success Rate by Security Mode")
    axes[0, 0].set_ylabel("Success Rate")
    axes[0, 0].set_ylim(0, 1)

    # Plot 2: Success Rate by Attack Type and Security Mode
    df_type = df_combined.groupby(["attack_type", "security_mode"])["success"].mean().reset_index()
    sns.barplot(x="attack_type", y="success", hue="security_mode", data=df_type, ax=axes[0, 1])
    axes[0, 1].set_title("Success Rate by Attack Type and Security Mode")
    axes[0, 1].set_ylabel("Success Rate")
    axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45, ha="right")
    axes[0, 1].set_ylim(0, 1)

    # Plot 3: Success Rate by Complexity and Security Mode
    df_complexity = df_combined.groupby(["complexity", "security_mode"])["success"].mean().reset_index()
    sns.barplot(x="complexity", y="success", hue="security_mode", data=df_complexity, ax=axes[1, 0])
    axes[1, 0].set_title("Success Rate by Complexity and Security Mode")
    axes[1, 0].set_ylabel("Success Rate")
    axes[1, 0].set_ylim(0, 1)

    # Plot 4: Success Rate by Target and Security Mode
    df_target = df_combined.groupby(["system_target", "security_mode"])["success"].mean().reset_index()
    sns.barplot(x="system_target", y="success", hue="security_mode", data=df_target, ax=axes[1, 1])
    axes[1, 1].set_title("Success Rate by Target and Security Mode")
    axes[1, 1].set_ylabel("Success Rate")
    axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45, ha="right")
    axes[1, 1].set_ylim(0, 1)

    # Adjust layout and save
    plt.tight_layout()
    plt.savefig(output_file)

    return fig


if __name__ == "__main__":
    main()